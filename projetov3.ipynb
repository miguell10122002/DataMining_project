{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset after removing duplicates: (12610, 26)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import researchpy as rp\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score,precision_score, recall_score, roc_auc_score, log_loss, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier, GradientBoostingClassifier,BaggingClassifier,StackingClassifier\n",
    "\n",
    "\n",
    "# Replace 'your_file.csv' with the path to your CSV file.\n",
    "csv_file = 'in-vehicle-coupon-recommendation.csv'\n",
    "\n",
    "# Load the CSV file into a pandas DataFrame.\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape of dataset after removing duplicates:\",df.shape)\n",
    "\n",
    "# Drop the 'car' , 'toCoupon_GEQ5min' and 'direction_opp' columns\n",
    "df = df.drop(['car', 'toCoupon_GEQ5min','direction_opp'], axis=1)\n",
    "\n",
    "weather_col = 'weather'\n",
    "temp_col = 'temperature'\n",
    "\n",
    "\n",
    "# Fill missing values with the mode (most common value) of each column\n",
    "df = df.fillna(df.mode().iloc[0])\n",
    "# Step 1: Calculate acceptance ratio for each occupation\n",
    "acceptance_ratio = df.groupby('occupation')['Y'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Bin occupations based on acceptance ratio\n",
    "# Using quantiles as dynamic boundaries. Adjust according to your needs.\n",
    "bins = [\n",
    "    acceptance_ratio.min(),\n",
    "    acceptance_ratio.quantile(0.2),\n",
    "    acceptance_ratio.quantile(0.4),\n",
    "    acceptance_ratio.quantile(0.6),\n",
    "    acceptance_ratio.quantile(0.8),\n",
    "    acceptance_ratio.max()\n",
    "]\n",
    "\n",
    "bin_labels = ['low', 'medium_low', 'medium', 'medium_high', 'high']\n",
    "\n",
    "# Assign bin labels\n",
    "occupation_bins = pd.cut(acceptance_ratio, bins=bins, labels=bin_labels, include_lowest=True)\n",
    "\n",
    "# Step 3: Map original occupation to occupation_class\n",
    "df['occupation'] = df['occupation'].map(occupation_bins.to_dict())\n",
    "\n",
    "\n",
    "# Define a function to combine the features into 'toCoupon'\n",
    "def combine_features(row):\n",
    "    if row['toCoupon_GEQ15min'] == 0:  # driving distance <= 15 min\n",
    "        return 0\n",
    "    elif row['toCoupon_GEQ25min'] == 0 :  # driving distance > 15 min and <= 25 min\n",
    "        return 1\n",
    "    else:  # driving distance > 25 min\n",
    "        return 2\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "df['toCoupon'] = df.apply(combine_features, axis=1)\n",
    "\n",
    "# Optionally, drop the original features\n",
    "df = df.drop(['toCoupon_GEQ15min', 'toCoupon_GEQ25min'], axis=1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.countplot(data=df, x='toCoupon', hue='Y', palette='viridis', edgecolor='black', linewidth=0.7)\n",
    "\n",
    "\n",
    "# Feature Extraction for 'passenger_destination' from 'time' and 'destination'\n",
    "df['time_destination'] = df['time'].astype(str) + \"_\" + df['destination'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'marital_hasChildren' from 'maritalStatus' and 'has_children'\n",
    "df['marital_hasChildren'] = df['maritalStatus'].astype(str) + \"_\" + df['has_children'].astype(str)\n",
    "\n",
    "# Feature Extraction for 'temperature_weather' from 'temperature' and 'weather'\n",
    "df['temperature_weather'] = df['temperature'].astype(str) + \"_\" + df['weather'].astype(str)\n",
    "\n",
    "\n",
    "df = df.drop(['time', 'destination', 'maritalStatus', 'has_children', 'temperature', 'weather'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Define order for the ordinal variables\n",
    "\n",
    "age_order = {'below21': 0, '21': 1, '26': 2, '31': 3, '36': 4, '41': 5, '46': 6, '50plus': 7}\n",
    "education_order = {'Some High School': 0, 'High School Graduate': 1, 'Some college - no degree': 2, 'Associates degree': 3, 'Bachelors degree': 4, 'Graduate degree (Masters or Doctorate)': 5}\n",
    "income_order = {'Less than $12500': 0, '$12500 - $24999': 1, '$25000 - $37499': 2, '$37500 - $49999': 3, '$50000 - $62499': 4, '$62500 - $74999': 5, '$75000 - $87499': 6, '$87500 - $99999': 7, '$100000 or More': 8}\n",
    "frequency_order = {'never': 0, 'less1': 1, '1~3': 2, '4~8': 3, 'gt8': 4}\n",
    "occupation_order= { 'medium_low':1, 'high':4, 'medium_high':3, 'low' :0,'medium':2}\n",
    "\n",
    "# Replace the values based on the order\n",
    "df['age'] = df['age'].replace(age_order)\n",
    "df['education'] = df['education'].replace(education_order)\n",
    "df['income'] = df['income'].replace(income_order)\n",
    "df['occupation']=df['occupation'].replace(occupation_order)\n",
    "\n",
    "# Encoding frequency-like features\n",
    "for col in ['Bar', 'CoffeeHouse', 'CarryAway', 'RestaurantLessThan20', 'Restaurant20To50']:\n",
    "    df[col] = df[col].replace(frequency_order)\n",
    "\n",
    "\n",
    "# 1. One-Hot Encoding\n",
    "onehot_cols = ['passanger', 'coupon', 'marital_hasChildren', 'temperature_weather', 'time_destination']\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')  \n",
    "encoded_cols = pd.DataFrame(encoder.fit_transform(df[onehot_cols]))\n",
    "\n",
    "# Reset indices to ensure alignment when concatenating\n",
    "encoded_cols.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Use appropriate column names for one-hot encoded columns\n",
    "encoded_cols.columns = encoder.get_feature_names_out(onehot_cols)\n",
    "\n",
    "# Concatenate the original dataframe and the one-hot encoded columns\n",
    "df = pd.concat([df, encoded_cols], axis=1)\n",
    "\n",
    "# Drop the original columns that were one-hot encoded\n",
    "df.drop(onehot_cols, axis=1, inplace=True)\n",
    "\n",
    "# 2. Binary Encoding\n",
    "df['expiration'] = df['expiration'].map({'2h': 0, '1d': 1})\n",
    "# Note: 'Y' and 'direction_same' are already binary, no encoding needed\n",
    "\n",
    "# 3. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['gender'] = label_encoder.fit_transform(df['gender'])  # 0 for Female and 1 for Male"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df.drop(\"Y\", axis=1)\n",
    "y = df[\"Y\"]\n",
    "X = X.rename(columns={'coupon_Restaurant(<20)': 'coupon_Restaurant(20)'})\n",
    "\n",
    "\n",
    "# Split data into 75% train+validation and 25% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Initialize a k-fold cross-validator (e.g., 5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune\n",
    "param_grid = [\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['lbfgs']\n",
    "    },\n",
    "    {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the base Logistic Regression model\n",
    "clf_logistic_regression = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(clf_logistic_regression, param_grid, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Use the best model to make predictions on the test set\n",
    "best_clf = grid_search.best_estimator_\n",
    "y_test_preds = best_clf.predict(X_test)\n",
    "# Evaluating the model\n",
    "\n",
    "# Accuracy\n",
    "print(\" clf_logistic_regression Validation Accuracy:\", accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "# Precision\n",
    "print(\"clf_logistic_regression Validation Precision:\", precision_score(y_test, y_test_preds))\n",
    "\n",
    "# Recall\n",
    "print(\"clf_logistic_regression Validation Recall:\", recall_score(y_test, y_test_preds))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob = best_clf.predict_proba(X_test)[:, 1]\n",
    "print(\"clf_logistic_regression ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "\n",
    "# Log Loss\n",
    "print(\"clf_logistic_regression Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
    "print(\"clf_logistic_regression AUC Score:\", auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for KNN\n",
    "knn_params = {\n",
    "    'n_neighbors': list(range(1, 21)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Initialize KNN classifier\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV for KNN\n",
    "grid_search_knn = GridSearchCV(clf_knn, knn_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_knn.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for KNN\n",
    "print(f\"KNN Best Parameters: {grid_search_knn.best_params_}\")\n",
    "print(f\"KNN Best Cross-Validation Accuracy: {grid_search_knn.best_score_}\")\n",
    "\n",
    "# Use the best KNN model to make predictions on the test set\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "y_test_preds_knn = best_knn.predict(X_test)\n",
    "\n",
    "# Evaluating the KNN model\n",
    "\n",
    "# Accuracy\n",
    "print(\"KNN Validation Accuracy:\", accuracy_score(y_test, y_test_preds_knn))\n",
    "\n",
    "# Precision\n",
    "print(\"KNN Validation Precision:\", precision_score(y_test, y_test_preds_knn))\n",
    "\n",
    "# Recall\n",
    "print(\"KNN Validation Recall:\", recall_score(y_test, y_test_preds_knn))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_knn = best_knn.predict_proba(X_test)[:, 1]\n",
    "print(\"KNN ROC-AUC:\", roc_auc_score(y_test, y_test_prob_knn))\n",
    "\n",
    "# Log Loss\n",
    "print(\"KNN Log Loss:\", log_loss(y_test, y_test_prob_knn))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_knn, tpr_knn, _ = roc_curve(y_test, y_test_prob_knn)\n",
    "print(\"KNN AUC Score:\", auc(fpr_knn, tpr_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for Decision Tree\n",
    "dt_params = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Decision Tree classifier\n",
    "clf_dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for Decision Tree\n",
    "grid_search_dt = GridSearchCV(clf_dt, dt_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_dt.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for Decision Tree\n",
    "print(f\"Decision Tree Best Parameters: {grid_search_dt.best_params_}\")\n",
    "print(f\"Decision Tree Best Cross-Validation Accuracy: {grid_search_dt.best_score_}\")\n",
    "\n",
    "# Use the best Decision Tree model to make predictions on the test set\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "y_test_preds_dt = best_dt.predict(X_test)\n",
    "\n",
    "# Evaluating the Decision Tree model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Decision Tree Validation Accuracy:\", accuracy_score(y_test, y_test_preds_dt))\n",
    "\n",
    "# Precision\n",
    "print(\"Decision Tree Validation Precision:\", precision_score(y_test, y_test_preds_dt))\n",
    "\n",
    "# Recall\n",
    "print(\"Decision Tree Validation Recall:\", recall_score(y_test, y_test_preds_dt))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_dt = best_dt.predict_proba(X_test)[:, 1]\n",
    "print(\"Decision Tree ROC-AUC:\", roc_auc_score(y_test, y_test_prob_dt))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Decision Tree Log Loss:\", log_loss(y_test, y_test_prob_dt))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_test_prob_dt)\n",
    "print(\"Decision Tree AUC Score:\", auc(fpr_dt, tpr_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classification (SVC) with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for SVC\n",
    "svc_params = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "# Initialize SVC classifier with RBF kernel\n",
    "clf_svc = SVC(kernel='rbf', probability=True, random_state=42) # probability=True to ensure we can use predict_proba later\n",
    "\n",
    "# Initialize GridSearchCV for SVC\n",
    "grid_search_svc = GridSearchCV(clf_svc, svc_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_svc.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for SVC\n",
    "print(f\"SVC Best Parameters: {grid_search_svc.best_params_}\")\n",
    "print(f\"SVC Best Cross-Validation Accuracy: {grid_search_svc.best_score_}\")\n",
    "\n",
    "# Use the best SVC model to make predictions on the test set\n",
    "best_svc = grid_search_svc.best_estimator_\n",
    "y_test_preds_svc = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluating the SVC model\n",
    "\n",
    "# Accuracy\n",
    "print(\"SVC Validation Accuracy:\", accuracy_score(y_test, y_test_preds_svc))\n",
    "\n",
    "# Precision\n",
    "print(\"SVC Validation Precision:\", precision_score(y_test, y_test_preds_svc))\n",
    "\n",
    "# Recall\n",
    "print(\"SVC Validation Recall:\", recall_score(y_test, y_test_preds_svc))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_svc = best_svc.predict_proba(X_test)[:, 1]\n",
    "print(\"SVC ROC-AUC:\", roc_auc_score(y_test, y_test_prob_svc))\n",
    "\n",
    "# Log Loss\n",
    "print(\"SVC Log Loss:\", log_loss(y_test, y_test_prob_svc))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_svc, tpr_svc, _ = roc_curve(y_test, y_test_prob_svc)\n",
    "print(\"SVC AUC Score:\", auc(fpr_svc, tpr_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for LinearSVC\n",
    "linear_svc_params = {\n",
    "    'estimator__C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Initialize LinearSVC classifier\n",
    "clf_linear_svc = LinearSVC(max_iter=10000, random_state=42)  # Increased max_iter for convergence\n",
    "\n",
    "# For probability outputs, wrap LinearSVC within CalibratedClassifierCV\n",
    "clf_calibrated = CalibratedClassifierCV(clf_linear_svc, method='sigmoid', cv=5)\n",
    "\n",
    "# Initialize GridSearchCV for LinearSVC\n",
    "grid_search_linear_svc = GridSearchCV(clf_calibrated, linear_svc_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_linear_svc.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for LinearSVC\n",
    "print(f\"LinearSVC Best Parameters: {grid_search_linear_svc.best_params_}\")\n",
    "print(f\"LinearSVC Best Cross-Validation Accuracy: {grid_search_linear_svc.best_score_}\")\n",
    "\n",
    "# Use the best LinearSVC model to make predictions on the test set\n",
    "best_linear_svc = grid_search_linear_svc.best_estimator_\n",
    "y_test_preds_linear_svc = best_linear_svc.predict(X_test)\n",
    "\n",
    "# Evaluating the LinearSVC model\n",
    "\n",
    "# Accuracy\n",
    "print(\"LinearSVC Validation Accuracy:\", accuracy_score(y_test, y_test_preds_linear_svc))\n",
    "\n",
    "# Precision\n",
    "print(\"LinearSVC Validation Precision:\", precision_score(y_test, y_test_preds_linear_svc))\n",
    "\n",
    "# Recall\n",
    "print(\"LinearSVC Validation Recall:\", recall_score(y_test, y_test_preds_linear_svc))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_linear_svc = best_linear_svc.predict_proba(X_test)[:, 1]\n",
    "print(\"LinearSVC ROC-AUC:\", roc_auc_score(y_test, y_test_prob_linear_svc))\n",
    "\n",
    "# Log Loss\n",
    "print(\"LinearSVC Log Loss:\", log_loss(y_test, y_test_prob_linear_svc))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_linear_svc, tpr_linear_svc, _ = roc_curve(y_test, y_test_prob_linear_svc)\n",
    "print(\"LinearSVC AUC Score:\", auc(fpr_linear_svc, tpr_linear_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for RandomForestClassifier\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for RandomForest\n",
    "grid_search_rf = GridSearchCV(clf_rf, rf_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for RandomForest\n",
    "print(f\"Random Forest Best Parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Random Forest Best Cross-Validation Accuracy: {grid_search_rf.best_score_}\")\n",
    "\n",
    "# Use the best RandomForest model to make predictions on the test set\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_test_preds_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluating the RandomForest model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Random Forest Validation Accuracy:\", accuracy_score(y_test, y_test_preds_rf))\n",
    "\n",
    "# Precision\n",
    "print(\"Random Forest Validation Precision:\", precision_score(y_test, y_test_preds_rf))\n",
    "\n",
    "# Recall\n",
    "print(\"Random Forest Validation Recall:\", recall_score(y_test, y_test_preds_rf))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_rf = best_rf.predict_proba(X_test)[:, 1]\n",
    "print(\"Random Forest ROC-AUC:\", roc_auc_score(y_test, y_test_prob_rf))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Random Forest Log Loss:\", log_loss(y_test, y_test_prob_rf))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_prob_rf)\n",
    "print(\"Random Forest AUC Score:\", auc(fpr_rf, tpr_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to tune for ExtraTreesClassifier\n",
    "et_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize ExtraTreesClassifier\n",
    "clf_et = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for ExtraTrees\n",
    "grid_search_et = GridSearchCV(clf_et, et_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_et.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for ExtraTrees\n",
    "print(f\"Extra Trees Best Parameters: {grid_search_et.best_params_}\")\n",
    "print(f\"Extra Trees Best Cross-Validation Accuracy: {grid_search_et.best_score_}\")\n",
    "\n",
    "# Use the best ExtraTrees model to make predictions on the test set\n",
    "best_et = grid_search_et.best_estimator_\n",
    "y_test_preds_et = best_et.predict(X_test)\n",
    "\n",
    "# Evaluating the ExtraTrees model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Extra Trees Validation Accuracy:\", accuracy_score(y_test, y_test_preds_et))\n",
    "\n",
    "# Precision\n",
    "print(\"Extra Trees Validation Precision:\", precision_score(y_test, y_test_preds_et))\n",
    "\n",
    "# Recall\n",
    "print(\"Extra Trees Validation Recall:\", recall_score(y_test, y_test_preds_et))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_et = best_et.predict_proba(X_test)[:, 1]\n",
    "print(\"Extra Trees ROC-AUC:\", roc_auc_score(y_test, y_test_prob_et))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Extra Trees Log Loss:\", log_loss(y_test, y_test_prob_et))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_et, tpr_et, _ = roc_curve(y_test, y_test_prob_et)\n",
    "print(\"Extra Trees AUC Score:\", auc(fpr_et, tpr_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HistGradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HistGradientBoosting Best Parameters: {'learning_rate': 0.1, 'max_depth': 15, 'max_iter': 200, 'min_samples_leaf': 10}\n",
      "HistGradientBoosting Best Cross-Validation Accuracy: 0.7534113968134359\n",
      "HistGradientBoosting Validation Accuracy: 0.7459562321598477\n",
      "HistGradientBoosting Validation Precision: 0.751684810782789\n",
      "HistGradientBoosting Validation Recall: 0.8182844243792325\n",
      "HistGradientBoosting ROC-AUC: 0.8181722522528414\n",
      "HistGradientBoosting Log Loss: 0.5128126128278717\n",
      "HistGradientBoosting AUC Score: 0.8181722522528414\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "# HistGradientBoosting\n",
    "\n",
    "# Define hyperparameters to tune for HistGradientBoosting\n",
    "hgb_params = {\n",
    "    'max_iter': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'min_samples_leaf': [5, 10, 20]\n",
    "}\n",
    "\n",
    "# Initialize HistGradientBoosting classifier\n",
    "clf_hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for HistGradientBoosting\n",
    "grid_search_hgb = GridSearchCV(clf_hgb, hgb_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_hgb.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for HistGradientBoosting\n",
    "print(f\"HistGradientBoosting Best Parameters: {grid_search_hgb.best_params_}\")\n",
    "print(f\"HistGradientBoosting Best Cross-Validation Accuracy: {grid_search_hgb.best_score_}\")\n",
    "\n",
    "# Use the best HistGradientBoosting model to make predictions on the test set\n",
    "best_hgb = grid_search_hgb.best_estimator_\n",
    "y_test_preds_hgb = best_hgb.predict(X_test)\n",
    "\n",
    "# Evaluating the HistGradientBoosting model\n",
    "\n",
    "# Accuracy\n",
    "print(\"HistGradientBoosting Validation Accuracy:\", accuracy_score(y_test, y_test_preds_hgb))\n",
    "\n",
    "# Precision\n",
    "print(\"HistGradientBoosting Validation Precision:\", precision_score(y_test, y_test_preds_hgb))\n",
    "\n",
    "# Recall\n",
    "print(\"HistGradientBoosting Validation Recall:\", recall_score(y_test, y_test_preds_hgb))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_hgb = best_hgb.predict_proba(X_test)[:, 1]\n",
    "print(\"HistGradientBoosting ROC-AUC:\", roc_auc_score(y_test, y_test_prob_hgb))\n",
    "\n",
    "# Log Loss\n",
    "print(\"HistGradientBoosting Log Loss:\", log_loss(y_test, y_test_prob_hgb))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_hgb, tpr_hgb, _ = roc_curve(y_test, y_test_prob_hgb)\n",
    "print(\"HistGradientBoosting AUC Score:\", auc(fpr_hgb, tpr_hgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Best Parameters: {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Gradient Boosting Best Cross-Validation Accuracy: 0.7576419626516169\n",
      "Gradient Boosting Validation Accuracy: 0.7519822391373295\n",
      "Gradient Boosting Validation Precision: 0.7664155005382132\n",
      "Gradient Boosting Validation Recall: 0.8036117381489842\n",
      "Gradient Boosting ROC-AUC: 0.82347212982381\n",
      "Gradient Boosting Log Loss: 0.6458122384444003\n",
      "Gradient Boosting AUC Score: 0.82347212982381\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune for Gradient Boosting Classifier\n",
    "gb_params = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting Classifier\n",
    "clf_gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for Gradient Boosting Classifier\n",
    "grid_search_gb = GridSearchCV(clf_gb, gb_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_gb.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for Gradient Boosting Classifier\n",
    "print(f\"Gradient Boosting Best Parameters: {grid_search_gb.best_params_}\")\n",
    "print(f\"Gradient Boosting Best Cross-Validation Accuracy: {grid_search_gb.best_score_}\")\n",
    "\n",
    "# Use the best Gradient Boosting model to make predictions on the test set\n",
    "best_gb = grid_search_gb.best_estimator_\n",
    "y_test_preds_gb = best_gb.predict(X_test)\n",
    "\n",
    "# Evaluating the Gradient Boosting model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Gradient Boosting Validation Accuracy:\", accuracy_score(y_test, y_test_preds_gb))\n",
    "\n",
    "# Precision\n",
    "print(\"Gradient Boosting Validation Precision:\", precision_score(y_test, y_test_preds_gb))\n",
    "\n",
    "# Recall\n",
    "print(\"Gradient Boosting Validation Recall:\", recall_score(y_test, y_test_preds_gb))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_gb = best_gb.predict_proba(X_test)[:, 1]\n",
    "print(\"Gradient Boosting ROC-AUC:\", roc_auc_score(y_test, y_test_prob_gb))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Gradient Boosting Log Loss:\", log_loss(y_test, y_test_prob_gb))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_test_prob_gb)\n",
    "print(\"Gradient Boosting AUC Score:\", auc(fpr_gb, tpr_gb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Best Parameters: {'learning_rate': 1, 'n_estimators': 150}\n",
      "AdaBoost Best Cross-Validation Accuracy: 0.6816125231009691\n",
      "AdaBoost Validation Accuracy: 0.6796701554075484\n",
      "AdaBoost Validation Precision: 0.6930091185410334\n",
      "AdaBoost Validation Recall: 0.7720090293453724\n",
      "AdaBoost ROC-AUC: 0.7340433617802391\n",
      "AdaBoost Log Loss: 0.691684513735615\n",
      "AdaBoost AUC Score: 0.7340433617802391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary library for AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define hyperparameters to tune for AdaBoost\n",
    "ada_params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Initialize AdaBoost classifier\n",
    "clf_ada = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for AdaBoost\n",
    "grid_search_ada = GridSearchCV(clf_ada, ada_params, cv=kf, scoring='accuracy')\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_ada.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for AdaBoost\n",
    "print(f\"AdaBoost Best Parameters: {grid_search_ada.best_params_}\")\n",
    "print(f\"AdaBoost Best Cross-Validation Accuracy: {grid_search_ada.best_score_}\")\n",
    "\n",
    "# Use the best AdaBoost model to make predictions on the test set\n",
    "best_ada = grid_search_ada.best_estimator_\n",
    "y_test_preds_ada = best_ada.predict(X_test)\n",
    "\n",
    "# Evaluating the AdaBoost model\n",
    "\n",
    "# Accuracy\n",
    "print(\"AdaBoost Validation Accuracy:\", accuracy_score(y_test, y_test_preds_ada))\n",
    "\n",
    "# Precision\n",
    "print(\"AdaBoost Validation Precision:\", precision_score(y_test, y_test_preds_ada))\n",
    "\n",
    "# Recall\n",
    "print(\"AdaBoost Validation Recall:\", recall_score(y_test, y_test_preds_ada))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_ada = best_ada.predict_proba(X_test)[:, 1]\n",
    "print(\"AdaBoost ROC-AUC:\", roc_auc_score(y_test, y_test_prob_ada))\n",
    "\n",
    "# Log Loss\n",
    "print(\"AdaBoost Log Loss:\", log_loss(y_test, y_test_prob_ada))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_ada, tpr_ada, _ = roc_curve(y_test, y_test_prob_ada)\n",
    "print(\"AdaBoost AUC Score:\", auc(fpr_ada, tpr_ada))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Best Parameters: {'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
      "Bagging Classifier Best Cross-Validation Accuracy: 0.7481240839270921\n",
      "Bagging Classifier Validation Accuracy: 0.750396447827466\n",
      "Bagging Classifier Validation Precision: 0.7623867874267448\n",
      "Bagging Classifier Validation Recall: 0.8075620767494357\n",
      "Bagging Classifier ROC-AUC: 0.8206247149724658\n",
      "Bagging Classifier Log Loss: 0.521428166357484\n",
      "Bagging Classifier AUC Score: 0.8206247149724658\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters to tune for Bagging Classifier\n",
    "bagging_params = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0],\n",
    "    'bootstrap': [True, False],\n",
    "    'bootstrap_features': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize Bagging classifier\n",
    "clf_bagging = BaggingClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV for Bagging Classifier\n",
    "grid_search_bagging = GridSearchCV(clf_bagging, bagging_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_bagging.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for Bagging Classifier\n",
    "print(f\"Bagging Classifier Best Parameters: {grid_search_bagging.best_params_}\")\n",
    "print(f\"Bagging Classifier Best Cross-Validation Accuracy: {grid_search_bagging.best_score_}\")\n",
    "\n",
    "# Use the best Bagging model to make predictions on the test set\n",
    "best_bagging = grid_search_bagging.best_estimator_\n",
    "y_test_preds_bagging = best_bagging.predict(X_test)\n",
    "\n",
    "# Evaluating the Bagging model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Bagging Classifier Validation Accuracy:\", accuracy_score(y_test, y_test_preds_bagging))\n",
    "\n",
    "# Precision\n",
    "print(\"Bagging Classifier Validation Precision:\", precision_score(y_test, y_test_preds_bagging))\n",
    "\n",
    "# Recall\n",
    "print(\"Bagging Classifier Validation Recall:\", recall_score(y_test, y_test_preds_bagging))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_bagging = best_bagging.predict_proba(X_test)[:, 1]\n",
    "print(\"Bagging Classifier ROC-AUC:\", roc_auc_score(y_test, y_test_prob_bagging))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Bagging Classifier Log Loss:\", log_loss(y_test, y_test_prob_bagging))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_bagging, tpr_bagging, _ = roc_curve(y_test, y_test_prob_bagging)\n",
    "print(\"Bagging Classifier AUC Score:\", auc(fpr_bagging, tpr_bagging))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Best Parameters: {'depth': 8, 'iterations': 1000, 'l2_leaf_reg': 1, 'learning_rate': 0.05}\n",
      "CatBoost Best Cross-Validation Accuracy: 0.7632445555502139\n",
      "CatBoost Validation Accuracy: 0.7573739295908658\n",
      "CatBoost Validation Precision: 0.7696839850026781\n",
      "CatBoost Validation Recall: 0.8109480812641083\n",
      "CatBoost ROC-AUC: 0.8338765542684252\n",
      "CatBoost Log Loss: 0.5186806950862224\n",
      "CatBoost AUC Score: 0.8338765542684252\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define hyperparameters to tune for CatBoost\n",
    "cb_params = {\n",
    "    'iterations': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8],\n",
    "    'l2_leaf_reg': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "clf_cb = CatBoostClassifier(random_seed=42, verbose=0) # verbose=0 will prevent it from printing intermediate steps\n",
    "\n",
    "# Initialize GridSearchCV for CatBoost\n",
    "grid_search_cb = GridSearchCV(clf_cb, cb_params, cv=kf, scoring='accuracy', n_jobs=-1)  # Using n_jobs=-1 to use all available cores\n",
    "\n",
    "# Perform GridSearch on training+validation set\n",
    "grid_search_cb.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Print best parameters and corresponding score for CatBoost\n",
    "print(f\"CatBoost Best Parameters: {grid_search_cb.best_params_}\")\n",
    "print(f\"CatBoost Best Cross-Validation Accuracy: {grid_search_cb.best_score_}\")\n",
    "\n",
    "# Use the best CatBoost model to make predictions on the test set\n",
    "best_cb = grid_search_cb.best_estimator_\n",
    "y_test_preds_cb = best_cb.predict(X_test)\n",
    "\n",
    "# Evaluating the CatBoost model\n",
    "\n",
    "# Accuracy\n",
    "print(\"CatBoost Validation Accuracy:\", accuracy_score(y_test, y_test_preds_cb))\n",
    "\n",
    "# Precision\n",
    "print(\"CatBoost Validation Precision:\", precision_score(y_test, y_test_preds_cb))\n",
    "\n",
    "# Recall\n",
    "print(\"CatBoost Validation Recall:\", recall_score(y_test, y_test_preds_cb))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_cb = best_cb.predict_proba(X_test)[:, 1]\n",
    "print(\"CatBoost ROC-AUC:\", roc_auc_score(y_test, y_test_prob_cb))\n",
    "\n",
    "# Log Loss\n",
    "print(\"CatBoost Log Loss:\", log_loss(y_test, y_test_prob_cb))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_cb, tpr_cb, _ = roc_curve(y_test, y_test_prob_cb)\n",
    "print(\"CatBoost AUC Score:\", auc(fpr_cb, tpr_cb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.25000000000000006, 'max_depth': 6, 'n_estimators': 207}\n",
      "XGBoost Best Cross-Validation Accuracy: 0.7593332386747954\n",
      "XGBoost Validation Accuracy: 0.7494449730415478\n",
      "XGBoost Validation Precision: 0.7634120171673819\n",
      "XGBoost Validation Recall: 0.8030474040632054\n",
      "XGBoost ROC-AUC: 0.8219642422231412\n",
      "XGBoost Log Loss: 0.5431885741903345\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jose/Desktop/MEDM/projetov3.ipynb Cell 30\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jose/Desktop/MEDM/projetov3.ipynb#X46sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mXGBoost Log Loss:\u001b[39m\u001b[39m\"\u001b[39m, log_loss(y_test, y_test_prob))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jose/Desktop/MEDM/projetov3.ipynb#X46sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#AUC Score (using roc_curve function)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/jose/Desktop/MEDM/projetov3.ipynb#X46sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m fpr_rf, tprrf,  \u001b[39m=\u001b[39m roc_curve(y_test, y_test_prob)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/jose/Desktop/MEDM/projetov3.ipynb#X46sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mXGBoost AUC Score:\u001b[39m\u001b[39m\"\u001b[39m, auc(fpr_rf, tpr_rf))\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Create an XGBoost classifier\n",
    "clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',  # Binary classification\n",
    "    use_label_encoder=False  # Suppress warning about label encoding\n",
    ")\n",
    "\n",
    "#Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [5, 6, 7],\n",
    "    'learning_rate': np.arange(0.20, 0.30, 0.01),\n",
    "    'n_estimators': np.arange(205, 209, 1),\n",
    "}\n",
    "\n",
    "#Perform hyperparameter tuning with cross-validation\n",
    "grid_search_xgb = GridSearchCV(estimator=clf, param_grid=param_grid,cv=kf, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_val, y_train_val)\n",
    "\n",
    "#Get the best hyperparameters from the grid search\n",
    "best_params = grid_search_xgb.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "#Train the XGBoost model with the best hyperparameters\n",
    "best_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    **best_params\n",
    ")\n",
    "best_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "#Make predictions\n",
    "y_test_preds = best_clf.predict(X_test)\n",
    "\n",
    "#Calculate the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, best_clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(f\"XGBoost Best Cross-Validation Accuracy: {grid_search_xgb.best_score_}\")\n",
    "\n",
    "#Accuracy\n",
    "print(\"XGBoost Validation Accuracy:\",\n",
    "      accuracy_score(y_test, y_test_preds))\n",
    "\n",
    "#Precision\n",
    "print(\"XGBoost Validation Precision:\",\n",
    "      precision_score(y_test, y_test_preds))\n",
    "\n",
    "#Recall\n",
    "print(\"XGBoost Validation Recall:\", recall_score(y_test, y_test_preds))\n",
    "\n",
    "#ROC-AUC for probability scores\n",
    "y_test_prob = best_clf.predict_proba(X_test)[:, 1]\n",
    "print(\"XGBoost ROC-AUC:\", roc_auc_score(y_test, y_test_prob))\n",
    "\n",
    "#Log Loss\n",
    "print(\"XGBoost Log Loss:\", log_loss(y_test, y_test_prob))\n",
    "\n",
    "#AUC Score (using roc_curve function)\n",
    "fpr_rf, tprrf,  = roc_curve(y_test, y_test_prob)\n",
    "print(\"XGBoost AUC Score:\", auc(fpr_rf, tpr_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Validation Accuracy: 0.7614969869965112\n",
      "Stacking Classifier Validation Precision: 0.7701271186440678\n",
      "Stacking Classifier Validation Recall: 0.8205417607223476\n",
      "Stacking Classifier ROC-AUC: 0.8373835167044524\n",
      "Stacking Classifier Log Loss: 0.49553352387760263\n",
      "Stacking Classifier AUC Score: 0.8373835167044524\n",
      "Stacking Classifier report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.69      0.72      1381\n",
      "           1       0.77      0.82      0.79      1772\n",
      "\n",
      "    accuracy                           0.76      3153\n",
      "   macro avg       0.76      0.75      0.76      3153\n",
      "weighted avg       0.76      0.76      0.76      3153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "# Best parameters for each model\n",
    "catboost_params = {'depth': 8, 'iterations': 1000, 'l2_leaf_reg': 1, 'learning_rate': 0.05}\n",
    "bagging_params = {'bootstrap': False, 'bootstrap_features': False, 'max_features': 1.0, 'max_samples': 0.5, 'n_estimators': 100}\n",
    "gradient_boosting_params = {'learning_rate': 0.1, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "hist_gradient_boosting_params = {'learning_rate': 0.1, 'max_depth': 15, 'max_iter': 200, 'min_samples_leaf': 10}\n",
    "random_forest_params = {'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 500}\n",
    "#xgboost_params = {'learning_rate': 0.22000000000000003, 'max_depth': 6, 'n_estimators': 207, 'objective': 'binary:logistic', 'use_label_encoder': False}  # Added XGBoost parameters\n",
    "\n",
    "# Base classifiers\n",
    "catboost = CatBoostClassifier(**catboost_params, random_seed=42, verbose=0)\n",
    "bagging = BaggingClassifier(**bagging_params, random_state=42)\n",
    "gradient_boosting = GradientBoostingClassifier(**gradient_boosting_params, random_state=42)\n",
    "hist_gradient_boosting = HistGradientBoostingClassifier(**hist_gradient_boosting_params, random_state=42)\n",
    "random_forest = RandomForestClassifier(**random_forest_params, random_state=42)\n",
    "#xgboost = XGBClassifier(**xgboost_params)  \n",
    "\n",
    "# Updated estimators list\n",
    "estimators = [\n",
    "    ('catboost', catboost),\n",
    "    ('bagging', bagging),\n",
    "    ('gradient_boosting', gradient_boosting),\n",
    "    ('hist_gradient_boosting', hist_gradient_boosting),\n",
    "    ('random_forest', random_forest),\n",
    "    #('xgboost', xgboost)  # Added XGBoost to the estimators list\n",
    "]\n",
    "stacking_clf = StackingClassifier(estimators=estimators, final_estimator=None, cv=kf, n_jobs=-1)\n",
    "\n",
    "# Assuming you've already defined your training and test sets as X_train_val, y_train_val and X_test, y_test respectively\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_preds_stacking = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluating the Stacking model\n",
    "\n",
    "# Accuracy\n",
    "print(\"Stacking Classifier Validation Accuracy:\", accuracy_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# Precision\n",
    "print(\"Stacking Classifier Validation Precision:\", precision_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# Recall\n",
    "print(\"Stacking Classifier Validation Recall:\", recall_score(y_test, y_test_preds_stacking))\n",
    "\n",
    "# ROC-AUC for probability scores\n",
    "y_test_prob_stacking = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "print(\"Stacking Classifier ROC-AUC:\", roc_auc_score(y_test, y_test_prob_stacking))\n",
    "\n",
    "# Log Loss\n",
    "print(\"Stacking Classifier Log Loss:\", log_loss(y_test, y_test_prob_stacking))\n",
    "\n",
    "# AUC Score (using roc_curve function)\n",
    "fpr_stacking, tpr_stacking, _ = roc_curve(y_test, y_test_prob_stacking)\n",
    "print(\"Stacking Classifier AUC Score:\", auc(fpr_stacking, tpr_stacking))\n",
    "print(\"Stacking Classifier report\", classification_report(y_test,y_test_preds_stacking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to model_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Create a dictionary to store the metrics\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'Classifier': ['Logistic Regression', 'KNN', 'Decision Tree', 'SVC', 'LinearSVC', 'Random Forest', 'Extra Trees','Hist Gradient Boosting','Gradient Boosting'],\n",
    "    'Best CV Accuracy': [\n",
    "        grid_search.best_score_,\n",
    "        grid_search_knn.best_score_,\n",
    "        grid_search_dt.best_score_,\n",
    "        grid_search_svc.best_score_,\n",
    "        grid_search_linear_svc.best_score_,\n",
    "        grid_search_rf.best_score_,\n",
    "        grid_search_et.best_score_,\n",
    "        grid_search_hgb.best_score_,\n",
    "        grid_search_gb.best_score_,\n",
    "        grid_search_ada.best_score_,\n",
    "        grid_search_bagging.best_score_,\n",
    "        grid_search_cb.best_score_\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        accuracy_score(y_test, y_test_preds),\n",
    "        accuracy_score(y_test, y_test_preds_knn),\n",
    "        accuracy_score(y_test, y_test_preds_dt),\n",
    "        accuracy_score(y_test, y_test_preds_svc),\n",
    "        accuracy_score(y_test, y_test_preds_linear_svc),\n",
    "        accuracy_score(y_test, y_test_preds_rf),\n",
    "        accuracy_score(y_test, y_test_preds_et),\n",
    "        accuracy_score(y_test, y_test_preds_hgb),\n",
    "        accuracy_score(y_test, y_test_preds_gb),\n",
    "        accuracy_score(y_test, y_test_preds_ada),\n",
    "        accuracy_score(y_test, y_test_preds_bagging),\n",
    "        accuracy_score(y_test, y_test_preds_cb)\n",
    "\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_test_preds),\n",
    "        precision_score(y_test, y_test_preds_knn),\n",
    "        precision_score(y_test, y_test_preds_dt),\n",
    "        precision_score(y_test, y_test_preds_svc),\n",
    "        precision_score(y_test, y_test_preds_linear_svc),\n",
    "        precision_score(y_test, y_test_preds_rf),\n",
    "        precision_score(y_test, y_test_preds_et),\n",
    "        precision_score(y_test, y_test_preds_hgb),\n",
    "        precision_score(y_test, y_test_preds_gb),\n",
    "        precision_score(y_test, y_test_preds_ada),\n",
    "        precision_score(y_test, y_test_preds_bagging),\n",
    "        precision_score(y_test, y_test_preds_cb)\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_test_preds),\n",
    "        recall_score(y_test, y_test_preds_knn),\n",
    "        recall_score(y_test, y_test_preds_dt),\n",
    "        recall_score(y_test, y_test_preds_svc),\n",
    "        recall_score(y_test, y_test_preds_linear_svc),\n",
    "        recall_score(y_test, y_test_preds_rf),\n",
    "        recall_score(y_test, y_test_preds_et),\n",
    "        recall_score(y_test, y_test_preds_hgb),\n",
    "        recall_score(y_test, y_test_preds_gb),\n",
    "        recall_score(y_test, y_test_preds_ada),\n",
    "        recall_score(y_test, y_test_preds_bagging),\n",
    "        recall_score(y_test, y_test_preds_cb)\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        roc_auc_score(y_test, y_test_prob),\n",
    "        roc_auc_score(y_test, y_test_prob_knn),\n",
    "        roc_auc_score(y_test, y_test_prob_dt),\n",
    "        roc_auc_score(y_test, y_test_prob_svc),\n",
    "        roc_auc_score(y_test, y_test_prob_linear_svc),\n",
    "        roc_auc_score(y_test, y_test_prob_rf),\n",
    "        roc_auc_score(y_test, y_test_prob_et),\n",
    "        roc_auc_score(y_test, y_test_prob_hgb),\n",
    "        roc_auc_score(y_test, y_test_prob_gb),\n",
    "        roc_auc_score(y_test, y_test_prob_ada),\n",
    "        roc_auc_score(y_test, y_test_prob_bagging),\n",
    "        roc_auc_score(y_test, y_test_prob_cb)\n",
    "    ],\n",
    "    'Log Loss': [\n",
    "        log_loss(y_test, y_test_prob),\n",
    "        log_loss(y_test, y_test_prob_knn),\n",
    "        log_loss(y_test, y_test_prob_dt),\n",
    "        log_loss(y_test, y_test_prob_svc),\n",
    "        log_loss(y_test, y_test_prob_linear_svc),\n",
    "        log_loss(y_test, y_test_prob_rf),\n",
    "        log_loss(y_test, y_test_prob_et),\n",
    "        log_loss(y_test, y_test_prob_hgb),\n",
    "        log_loss(y_test, y_test_prob_gb),\n",
    "        log_loss(y_test, y_test_prob_ada),\n",
    "        log_loss(y_test, y_test_prob_bagging),\n",
    "        log_loss(y_test, y_test_prob_cb)\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(metrics)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('model_metrics.csv', index=False)\n",
    "\n",
    "print(\"Metrics saved to model_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
